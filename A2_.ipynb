{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.0+cpu\n",
      "TorchText version: 0.16.2+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"TorchText version:\", torchtext.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"saracandu/harry-potter-trivia-human\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'text'],\n",
      "        num_rows: 1023\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'text'],\n",
      "        num_rows: 256\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: No validation set like LSTM code in the youtube video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What are the three European wizarding schools that participate in the Triwizard Tournament?', 'answer': 'Hogwarts, Beauxbatons, and Durmstrang.', 'text': '<s>[INST] What are the three European wizarding schools that participate in the Triwizard Tournament? [/INST] Hogwarts, Beauxbatons, and Durmstrang. </s>'}\n"
     ]
    }
   ],
   "source": [
    "print(ds['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<s>[INST] What are the three European wizarding schools that participate in the Triwizard Tournament? [/INST] Hogwarts, Beauxbatons, and Durmstrang. </s>', 'question_tokens': ['what', 'are', 'the', 'three', 'european', 'wizarding', 'schools', 'that', 'participate', 'in', 'the', 'triwizard', 'tournament', '?'], 'answer_tokens': ['hogwarts', ',', 'beauxbatons', ',', 'and', 'durmstrang', '.']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"saracandu/harry-potter-trivia-human\")\n",
    "\n",
    "# Tokenizer for basic English text\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Define a function to tokenize the 'question' and 'answer' fields\n",
    "def tokenize_data(example):\n",
    "    # Tokenizing both the 'question' and 'answer' fields\n",
    "    return {'question_tokens': tokenizer(example['question']),\n",
    "            'answer_tokens': tokenizer(example['answer'])}\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_dataset = ds.map(lambda x: tokenize_data(x), remove_columns=['question', 'answer'])\n",
    "\n",
    "# Check tokenized dataset\n",
    "print(tokenized_dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: The text field seems to have special tokens!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 743\n",
      "First 10 items in the vocabulary: ['<unk>', '<eos>', '?', 'the', 'what', 'of', 'is', '.', 'a', \"'\"]\n",
      "743\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Combine the 'question_tokens' and 'answer_tokens' from the tokenized dataset\n",
    "# to build a vocabulary that accounts for both question and answer tokens.\n",
    "def generate_tokens(dataset):\n",
    "    for example in dataset:\n",
    "        # Yield tokens from both 'question_tokens' and 'answer_tokens'\n",
    "        yield example['question_tokens']\n",
    "        yield example['answer_tokens']\n",
    "\n",
    "# Build the vocabulary from both the 'question_tokens' and 'answer_tokens'\n",
    "vocab = build_vocab_from_iterator(generate_tokens(tokenized_dataset['train']), min_freq=3)\n",
    "\n",
    "# Insert special tokens\n",
    "vocab.insert_token('<unk>', 0)  # Unknown token index\n",
    "vocab.insert_token('<eos>', 1)  # End-of-sequence token index\n",
    "vocab.set_default_index(vocab['<unk>'])  # Default index for unknown tokens\n",
    "\n",
    "# Check the vocabulary length and the first 10 items in the vocabulary\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"First 10 items in the vocabulary: {vocab.get_itos()[:10]}\")\n",
    "\n",
    "\n",
    "# convert 'question_tokens' and 'answer_tokens' into numerical indices\n",
    "numericalized_question = [vocab[token] for token in tokenized_dataset['train'][0]['question_tokens']]\n",
    "numericalized_answer = [vocab[token] for token in tokenized_dataset['train'][0]['answer_tokens']]\n",
    "\n",
    "print(len(vocab))\n",
    "torch.save(vocab, 'vocab.pth')  # Save the vocab for webapp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prepare the batch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []  # List to hold the numericalized tokens\n",
    "    \n",
    "    for example in dataset:\n",
    "        if example['question_tokens'] and example['answer_tokens']:  # all fields are present\n",
    "            # Append <eos> token to both question and answer tokens\n",
    "            question_tokens = example['question_tokens'] + ['<eos>']\n",
    "            answer_tokens = example['answer_tokens'] + ['<eos>']\n",
    "            \n",
    "            # Combine question and answer tokens into one sequence\n",
    "            tokens = question_tokens + answer_tokens\n",
    "            \n",
    "            # Numericalize the tokens\n",
    "            tokens = [vocab[token] for token in tokens]\n",
    "            \n",
    "            # Add the numericalized tokens to our data list\n",
    "            data.extend(tokens)\n",
    "    \n",
    "    # Convert the data to a LongTensor\n",
    "    data = torch.LongTensor(data)\n",
    "    \n",
    "    # Number of complete batches that can make from the data\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    \n",
    "    # Truncate the data to ensure it fits into full batches\n",
    "    data = data[:num_batches * batch_size]\n",
    "    \n",
    "    # Reshape data into the shape [batch_size, num_batches]\n",
    "    data = data.view(batch_size, num_batches)\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Assuming the tokenized dataset has only 'train' and 'test'\\nbatch_size = 128\\n\\n# Prepare the data for train, validation (use test as validation), and test\\ntrain_data = get_data(tokenized_dataset['train'], vocab, batch_size)\\nvalid_data = get_data(tokenized_dataset['test'], vocab, batch_size)  # Using test for validation\\n# In this case, there is no 'validation' split, so using 'test' as validation\\ntest_data  = valid_data  # set test_data to be the same as valid_data for now\\n\\n# Check the shape of the train data\\nprint(train_data.shape)  \""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Assuming the tokenized dataset has only 'train' and 'test'\n",
    "batch_size = 128\n",
    "\n",
    "# Prepare the data for train, validation (use test as validation), and test\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['test'], vocab, batch_size)  # Using test for validation\n",
    "# In this case, there is no 'validation' split, so using 'test' as validation\n",
    "test_data  = valid_data  # set test_data to be the same as valid_data for now\n",
    "\n",
    "# Check the shape of the train data\n",
    "print(train_data.shape)  \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 861])\n",
      "torch.Size([15, 213])\n",
      "torch.Size([15, 266])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "batch_size = 15\n",
    "# Get the indices for the dataset\n",
    "train_size = len(tokenized_dataset['train'])\n",
    "train_indices = list(range(train_size))\n",
    "\n",
    "# Split the indices into train and validation\n",
    "train_idx, valid_idx = train_test_split(train_indices, test_size=0.2)\n",
    "\n",
    "# Select the data corresponding to these indices\n",
    "train_tokens = tokenized_dataset['train'].select(train_idx)\n",
    "valid_tokens = tokenized_dataset['train'].select(valid_idx)\n",
    "\n",
    "# Prepare the data for train, validation, and test\n",
    "train_data = get_data(train_tokens, vocab, batch_size)\n",
    "valid_data = get_data(valid_tokens, vocab, batch_size)\n",
    "test_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)  \n",
    "print(test_data.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.hid_dim   = hid_dim\n",
    "        self.num_layers= num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm      = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers,\n",
    "                                 dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout   = nn.Dropout(dropout_rate)\n",
    "        self.fc        = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        embed = self.embedding(src)  # [batch_size, seq_len, emb_dim]\n",
    "        output, hidden = self.lstm(embed, hidden)  # LSTM output\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)  # [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 18,316,007 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "batch_size = 15\n",
    "lr = 1e-3\n",
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.30          \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    # Given data from get_data() - src and target are offset by 1\n",
    "    src = data[:, idx:idx+seq_len]                    \n",
    "    target = data[:, idx+1:idx+seq_len+1]  # Target is shifted by 1 from source\n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ', leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx)  # src, target: [batch_size, seq_len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        # Forward pass through the model\n",
    "        prediction, hidden = model(src, hidden)\n",
    "\n",
    "        # Reshaping prediction for cross-entropy loss (batch_size * seq_len, vocab_size)\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(prediction, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # Prevent gradient explosion\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size = src.shape[0]\n",
    "\n",
    "            # Forward pass through the model\n",
    "            prediction, hidden = model(src, hidden)\n",
    "\n",
    "            # Reshaping prediction for cross-entropy loss (batch_size * seq_len, vocab_size)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(prediction, target)\n",
    "\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 138.599\n",
      "\tValid Perplexity: 76.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 57.460\n",
      "\tValid Perplexity: 47.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.167\n",
      "\tValid Perplexity: 36.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 27.408\n",
      "\tValid Perplexity: 30.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 20.908\n",
      "\tValid Perplexity: 26.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 16.167\n",
      "\tValid Perplexity: 24.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 13.209\n",
      "\tValid Perplexity: 22.216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 10.935\n",
      "\tValid Perplexity: 20.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 9.032\n",
      "\tValid Perplexity: 19.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 7.636\n",
      "\tValid Perplexity: 18.362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 6.492\n",
      "\tValid Perplexity: 17.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 5.587\n",
      "\tValid Perplexity: 17.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 4.942\n",
      "\tValid Perplexity: 17.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 4.308\n",
      "\tValid Perplexity: 19.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 3.751\n",
      "\tValid Perplexity: 18.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 3.384\n",
      "\tValid Perplexity: 18.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 3.113\n",
      "\tValid Perplexity: 19.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.861\n",
      "\tValid Perplexity: 19.568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.709\n",
      "\tValid Perplexity: 19.391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.567\n",
      "\tValid Perplexity: 19.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.448\n",
      "\tValid Perplexity: 19.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.397\n",
      "\tValid Perplexity: 19.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.346\n",
      "\tValid Perplexity: 20.061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.274\n",
      "\tValid Perplexity: 20.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.227\n",
      "\tValid Perplexity: 20.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.205\n",
      "\tValid Perplexity: 20.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.169\n",
      "\tValid Perplexity: 20.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.163\n",
      "\tValid Perplexity: 20.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.148\n",
      "\tValid Perplexity: 20.770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 2.134\n",
      "\tValid Perplexity: 20.811\n"
     ]
    }
   ],
   "source": [
    "seq_len = 50\n",
    "clip = 0.25\n",
    "n_epochs = 30\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\n",
    "\n",
    "# Training loop\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train the model\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    # Save the model with the best validation loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "    # Print training and validation perplexity\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclutions: \n",
    "Training Perplexity is consistently decreasing. This means model is learning the training data well.\n",
    "Validation Perplexity initially decreases, then stagnates and slightly increases. This means model is memorizing the training data instead of generalizing to unseen data(overfitting).Early Stopping can be solved this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 13.870\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Real-world inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['harry',\n",
       " 'potter',\n",
       " 'is',\n",
       " 'this',\n",
       " 'department',\n",
       " 'professor',\n",
       " 'voldemort',\n",
       " 'maladies',\n",
       " 'and',\n",
       " 'mad-eye',\n",
       " 'north',\n",
       " 'headmaster',\n",
       " 'once',\n",
       " 'seven',\n",
       " ',',\n",
       " 'and',\n",
       " 'her',\n",
       " 'filius',\n",
       " 'called',\n",
       " '.',\n",
       " 'severus',\n",
       " 'piece',\n",
       " 'hospital']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"Harry Potter is\", 20, 2, model, tokenizer, vocab, device, seed=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
